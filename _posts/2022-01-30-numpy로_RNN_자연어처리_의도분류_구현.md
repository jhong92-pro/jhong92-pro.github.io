---
title: "numpy로 RNN 자연어처리 의도분류 구현 - 선형대수, 미적분"
categories:
  - 인공지능
tags:
  - numpy
use_math: true
---


![RNN 위키](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)  
numpy로 RNN을 만드려니 선형대수에서 막히는 부분이 많았습니다.  
제가 알게된 부분, 막혔던 부분 위주로 공유해 드리겠습니다.  
아직도 모르는 부분이 있긴 한데 그 부분은 나중에 채우도록 하겠습니다.^^  
  
설명은 쭉 numerator layout convention`(분자가 열로, 분모를 행으로)`을 따르도록 하겠습니다.  
denominator layout convention로 계산하면 결과가 달라질 수 있습니다.  
  
## 벡터 미분 Chain rule
딥러닝에서 back propagation 시 중요한 개념입니다. 잠깐 짚고 넘어가겠습니다.  

$$
\frac{\partial f}{\partial \textbf{x}} = \left[ \frac{\partial f}{\partial x_1} \;\; \frac{\partial f}{\partial x_2}  \right]
$$ 
   
$$
\frac{\partial f}{\partial \textbf{y}} = \left[ \frac{\partial f}{\partial y_1} \;\; \frac{\partial f}{\partial y_2} \;\; \frac{\partial f}{\partial y_3}  \right]
$$  
  
$$
\frac{\partial \textbf{y}}{\partial \textbf{x}} = 
\begin{bmatrix}
\frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} \\ 
\frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} \\ 
\frac{\partial y_3}{\partial x_1} & \frac{\partial y_3}{\partial x_2}
\end{bmatrix}
$$  
  
$$
\frac{\partial f}{\partial x_1}
=
\frac{\partial f}{\partial y_1}\frac{\partial y_1}{\partial x_1} + 
\frac{\partial f}{\partial y_2}\frac{\partial y_2}{\partial x_1} + 
\frac{\partial f}{\partial y_3}\frac{\partial y_3}{\partial x_1}
$$  
  
$$
\frac{\partial f}{\partial x_2}
=
\frac{\partial f}{\partial y_1}\frac{\partial y_1}{\partial x_2} + 
\frac{\partial f}{\partial y_2}\frac{\partial y_2}{\partial x_2} + 
\frac{\partial f}{\partial y_3}\frac{\partial y_3}{\partial x_2}
$$ 
  
이기 때문에  
  
$$
\frac{\partial f}{\partial \textbf{x}} = \frac{\partial f}{\partial \textbf{y}}\frac{\partial \textbf{y}}{\partial \textbf{x}}
$$
  
순서는 꼭 지켜주어야 합니다.  

## 벡터를 행렬로 미분

weight에 대한 back propagation을 할 때 벡터를 행렬로 미분할 일이 생깁니다.  
벡터를 행렬로 미분하는 것을 다루는 곳이 많지 않아 정확히 맞는 지는 잘 모르겠지만 여러번 테스트 결과 연산결과는 맞습니다.  
$$W$$의 크기가 (m,n), $$\textbf{y}$$의 크기가 k라고 했을 때 $$\frac{\partial \textbf{y}}{\partial W}$$ 의 모양은 (k,n,m)가 됩니다. 

구하는 방법은 행렬을 스칼라로 취급하고 미분한 뒤 나머지 행렬을 미분하면 됩니다.

$$
\frac{\partial \textbf{y}}{\partial W} =
\begin{bmatrix}
\frac{\partial y_1}{\partial W} \\ 
\frac{\partial y_2}{\partial W} \\ 
\vdots \\
\frac{\partial y_k}{\partial W}
\end{bmatrix}
$$

여기서 각 요소는 스칼라를 행렬에 대해 미분한 것이므로

$$
\frac{\partial y_i}{\partial W}
= 
\begin{bmatrix}
\frac{\partial y_i}{\partial W_{11}} & \frac{\partial y_i}{\partial W_{21}} & \dots & \frac{\partial y_i}{\partial W_{m1}} \\ 
\frac{\partial y_i}{\partial W_{12}} & \ddots & & \vdots \\ 
\vdots \\
\frac{\partial y_i}{\partial W_{1n}} & \dots && \frac{\partial y_i}{\partial W_{mn}}
\end{bmatrix}
$$

즉 (k,n,m) 모양으로 나옵니다.  
<br/>  
저도 자세히는 모르겠지만 미분 시 numerator layout convention은 분자에 대해 먼저 편미분 후 분모 편미분, denominator layout convention은 분모를 먼저 편미분하고 분자를 편미분하는 것 같습니다.  


## tanh 미분
벡터를 벡터로 미분한 것이므로 모양은 (m, m)입니다

$$
\frac{\partial tanh(\textbf{x})}{\partial \textbf{x}}
\newline
=
\begin{bmatrix}
\frac{\partial tanh(x_1)}{\partial x_1} & \frac{\partial tanh(x_1)}{\partial x_2} & \dots & \frac{\partial tanh(x_1)}{\partial x_m}\\ 
\frac{\partial tanh(x_2)}{\partial x_1} & \ddots & & \vdots \\ 
\vdots \\
\frac{\partial tanh(x_m)}{\partial x_1} & \dots && \frac{\partial tanh(x_m)}{\partial x_m}
\end{bmatrix} 
\newline
=
\begin{bmatrix}
1-tanh^2(x_1) & 0 & \dots & 0\\ 
0 & \ddots & & \vdots \\ 
\vdots \\
0 & \dots && 1-tanh^2(x_m)
\end{bmatrix}
$$

softmax 및 Loss 미분은 넘어가도록 하겠습니다  

## RNN BackPropagation 계산
다음과 같이 의도분류하는 자연어처리 모델을 RNN을 통하여 만들어보도록 하겠습니다  
이번 글에서 워드 임베딩에 대한 자세한 내용은 최대한 다루지 않겠습니다.  
![자연어처리흐름](/assets/img/인공지능/자연어처리흐름.png)  
H에 대한 activation은 tanh, y에 대한 activation은 softmax 입니다  
<br/>
back propagation는 모든 Weight와 Bias를 업데이트하는 과정입니다.  
<br/>  
Wh,Wx 및 Bh는 시퀀스의 길이만큼 여러번 연산됩니다.  

$$
\begin{align}
& W_{h.new} = W_h- \alpha(\frac{\partial L}{\partial h_{in1}}\frac{\partial h_{in1}}{\partial W_h}+\frac{\partial L}{\partial h_{in2}}\frac{\partial h_{in2}}{\partial W_h})
\\\\
& W_{x.new} = W_x- \alpha(\frac{\partial L}{\partial h_{in0}}\frac{\partial h_{in0}}{\partial W_x}+\frac{\partial L}{\partial h_{in1}}\frac{\partial h_{in1}}{\partial W_x}+\frac{\partial L}{\partial h_{in2}}\frac{\partial h_{in2}}{\partial W_x})
\\\\
& B_{h.new} = B_h- \alpha(\frac{\partial L}{\partial h_{in1}}\frac{\partial h_{in1}}{\partial B_h}+\frac{\partial L}{\partial h_{in2}}\frac{\partial h_{in2}}{\partial B_h})
\end{align}
$$
  
    
<br/>  

**Activation backpropagation**  
$$\frac{\partial tanh(\textbf{x})}{\partial \textbf{x}}$$ 의 크기는 (m, m)이지만  
벡터와 곱셈을 한다면 굳이 (m, m)으로 차원을 팽창시켜주지 않아도 됩니다.  

$$
\begin{align}
\textbf{v}\frac{\partial tanh(\textbf{x})}{\partial \textbf{x}}
&=
\begin{bmatrix}
v_1 & v_2 & \dots & v_m
\end{bmatrix}
\begin{bmatrix}
1-tanh^2(x_1) & 0 & \dots & 0\\ 
0 & \ddots & & \vdots \\ 
\vdots \\
0 & \dots && 1-tanh^2(x_m)
\end{bmatrix}
\\\\
&=\begin{bmatrix}
v_1(1-tanh^2(x_1)) & v_2(1-tanh^2(x_2)) & \dots & v_m(1-tanh^2(x_m))
\end{bmatrix}
\\\\
&=
\textbf{v}* (1-tanh^2(\textbf{x}))
\end{align}
$$
  
`*는 요소끼리의 곱`  
<br/>

그래서 다른 분들이 activation 미분을 numpy로 구현해놓은 코드를 보면
```python
def sigmoid_back(x):
  sig = sigmoid(x)
  return sig * (1 - sig)

def tanh_back(x):
  t = tanh(x)
  return 1 - t*t
```  
위와 같이 벡터로 함수를 구현합니다.  
<br/> 

**Weight backpropagation**  
  
$$\frac{\partial h_{in}}{\partial W_h}$$를 numpy로 직접 구해서 $$\frac{\partial L}{\partial h_{in}}$$와 곱해주려고 하면  
$$\frac{\partial h_{in}}{\partial W_h}$$의 파라미터가 너무 많아서`(n*m*k 개)` 연산 시 시간이 오래걸립니다.  
벡터와 곱한다면 차원이 하나 줄어들기 때문에 곱셈을 먼저한 뒤 코딩을 해야 합니다.  
<br/>
$$\frac{\partial L}{\partial W_y}$$를 구할 때를 예를 들어 보겠습니다.  
보기 어려울 수 있으니 $$y_{in}$$을 $$y$$, $$h_{out2}$$를 $$h$$로 표기해서 구해보겠습니다.  
참고로 W의 모양은 (m,n)이고 따라서 y의 모양도 (m)입니다
  
Bias는 편미분할 때 고려하지 않아도 되니 $$W\textbf{h} = \textbf{y}$$로 두면  

$$
\begin{align}
W h = \textbf{y}
&= \begin{bmatrix}
W_{11}h_1+W_{12}h_2+\dots+ W_{1n}h_n\\ 
W_{21}h_1+W_{22}h_2+\dots+ W_{2n}h_n\\ 
\vdots \\
W_{m1}h_1+W_{m2}h_2+\dots+ W_{mn}h_n
\end{bmatrix}
\\\\
y_i&=W_{i1}h_1+W_{i2}h_2+\dots+ W_{in}h_n
\\\\
\frac{\partial y_i}{\partial W} 
&= 
\begin{bmatrix}
\frac{\partial y_i}{\partial W_{11}} & \frac{\partial y_i}{\partial W_{21}} & \dots & \frac{\partial y_i}{\partial W_{m1}} \\ 
\frac{\partial y_i}{\partial W_{12}} & \ddots & & \vdots \\ 
\vdots \\
\frac{\partial y_i}{\partial W_{1n}} & \dots && \frac{\partial y_i}{\partial W_{mn}}
\end{bmatrix}
\\\\
&=
\begin{bmatrix}
0 &  & \dots & 0 \\ 
\vdots & & & \vdots \\ 
0 &  & \dots & 0 \\ 
h_1 & h_2 & \dots & h_n &  \,-\, i번째 row\\
0 &  & \dots & 0 \\ 
\vdots & & & \vdots \\ 
0 &  & \dots & 0 \\ 
\end{bmatrix}
\end{align}
$$

$$\frac{\partial L}{\partial W_y}$$로 돌아가서 계산해주면

$$
\begin{align}
\frac{\partial L}{\partial W}
&= \frac{\partial L}{\partial \textbf{y}}\frac{\partial \textbf{y}}{\partial W}
\\\\
&=
\begin{bmatrix}
\frac{\partial L}{\partial y_1}&\frac{\partial L}{\partial y_2}&\dots&\frac{\partial L}{\partial y_m}
\end{bmatrix}
\begin{bmatrix}
\frac{\partial y_1}{\partial W} \\ 
\frac{\partial y_2}{\partial W} \\ 
\vdots \\
\frac{\partial y_m}{\partial W}
\end{bmatrix}
\\\\
&=\begin{bmatrix}
\frac{\partial L}{\partial y_1}\frac{\partial y_1}{\partial W} +
\frac{\partial L}{\partial y_2}\frac{\partial y_2}{\partial W} +
\dots+
\frac{\partial L}{\partial y_m}\frac{\partial y_m}{\partial W}
\end{bmatrix}\\\\
&=
\begin{bmatrix}
\frac{\partial L}{\partial y_1}h_1&\frac{\partial L}{\partial y_1}h_2&\dots&\frac{\partial L}{\partial y_1}h_n \\
\frac{\partial L}{\partial y_2}h_1&\frac{\partial L}{\partial y_2}h_2&\dots&\frac{\partial L}{\partial y_2}h_n \\
\vdots& & &\vdots \\
\frac{\partial L}{\partial y_m}h_1&\frac{\partial L}{\partial y_m}h_2&\dots&\frac{\partial L}{\partial y_m}h_n \\
\end{bmatrix}
\end{align}
$$
<br/>  
다음에는 직접 코드 구현까지 해보겠습니다.  
어디까지 다뤄야 할 지 잘 몰라서 설명이 부실한 부분이 많은 것 같은데 보충할 부분 있으면 말해주세요~~  

<!-- 
행렬을 먼저 미분하면 각 요소가 벡터가 되는데 두 식이 같은지는 잘 와닿지 않습니다.    

$$
\frac{\partial \textbf{y}}{\partial W} =
\begin{bmatrix}
\frac{\partial \textbf{y}}{\partial W_{11}} & \frac{\partial \textbf{y}}{\partial W_{21}} & \dots & \frac{\partial \textbf{y}}{\partial W_{m1}} \\ 
\frac{\partial \textbf{y}}{\partial W_{12}} & \ddots & & \vdots \\ 
\vdots \\
\frac{\partial \textbf{y}}{\partial W_{1n}} & \dots && \frac{\partial \textbf{y}}{\partial W_{mn}}
\end{bmatrix}
$$

여기서 각 요소는 벡터를 스칼라에 대해 미분한 것이므로

$$
\frac{\partial \textbf{y}}{\partial W_{ij}}
= 
\begin{bmatrix}
\frac{\partial y_1}{\partial W_{ij}} \\
\frac{\partial y_2}{\partial W_{ij}} \\
\vdots \\
\frac{\partial y_k}{\partial W_{ij}} \\
\end{bmatrix}
$$



$$
\begin{align}
\frac{\partial L}{\partial \textbf{y}}\frac{\partial \textbf{y}}{\partial W} 
& =
\begin{bmatrix}
\frac{\partial L}{\partial y_1} & \frac{\partial L}{\partial y_2} & \dots \frac{\partial L}{\partial y_n}
\end{bmatrix}
\begin{bmatrix}
\frac{\partial \textbf{y}}{\partial W_{11}} & \frac{\partial \textbf{y}}{\partial W_{21}} & \dots & \frac{\partial \textbf{y}}{\partial W_{m1}} \\ 
\frac{\partial \textbf{y}}{\partial W_{12}} & \ddots & & \vdots \\ 
\vdots \\
\frac{\partial \textbf{y}}{\partial W_{1n}} & \dots && \frac{\partial \textbf{y}}{\partial W_{mn}}
\end{bmatrix}
\\\\
& =\begin{bmatrix}
   \begin{pmatrix}
    \frac{\partial L}{\partial y_{1}}\frac{\partial \textbf{y}}{\partial W_{11}} \\
    + \\
    \frac{\partial L}{\partial y_{2}}\frac{\partial \textbf{y}}{\partial W_{12}} \\
    + \\
   \vdots \\
    + \\
    \frac{\partial L}{\partial y_{n}}\frac{\partial \textbf{y}}{\partial W_{1n}} \\
   \end{pmatrix}
&
   \begin{pmatrix}
    \frac{\partial L}{\partial y_{1}}\frac{\partial \textbf{y}}{\partial W_{21}} \\
    + \\
    \frac{\partial L}{\partial y_{2}}\frac{\partial \textbf{y}}{\partial W_{22}} \\
    + \\
   \vdots \\
    + \\
    \frac{\partial L}{\partial y_{n}}\frac{\partial \textbf{y}}{\partial W_{2n}} \\
   \end{pmatrix}
&
\dots
&
   \begin{pmatrix}
    \frac{\partial L}{\partial y_{1}}\frac{\partial \textbf{y}}{\partial W_{m1}} \\
    + \\
    \frac{\partial L}{\partial y_{2}}\frac{\partial \textbf{y}}{\partial W_{m2}} \\
    + \\
   \vdots \\
    + \\
    \frac{\partial L}{\partial y_{n}}\frac{\partial \textbf{y}}{\partial W_{mn}} \\
   \end{pmatrix}
\end{bmatrix}
\end{align}
$$

-->