---
title: "numpy로 RNN 자연어처리 의도분류 구현 - numpy"
categories:
  - 인공지능
tags:
  - numpy
use_math: true
---


## RNN 모델
[https://github.com/songys/Chatbot_data](https://github.com/songys/Chatbot_data)

최대한 단순한 데이터셋을 찾아보니 위와 같은 게 있더라고요.  
다음카페의 글과 답변, 글에 대한 레이블링이 되어있는 데이터입니다.   
- 챗봇 트레이닝용 문답 페어 11,876개
- 일상다반사 0, 이별(부정) 1, 사랑(긍정) 2로 레이블링
<br/>
이 데이터셋으로 의도분류하는 자연어처리 모델을 RNN을 통하여 만들어보도록 하겠습니다.  
  
pandas를 이용해서 데이터를 불러와줍니다  
```python
import pandas as pd
data_path = '######'
data = pd.read_csv(data_path)
data.head()
```
---표---  

| Q | A | label |
|:------|:---:|:---:|
|12시 땡!	|하루가 또 가네요.|	0|
|1지망 학교 떨어졌어	|위로해 드립니다.|	0|
|3박4일 놀러가고 싶다|	여행은 언제나 좋죠.|	0|
|3박4일 정도 놀러가고 싶다|	여행은 언제나 좋죠.|	0|
|PPL 심하네|	눈살이 찌푸려지죠.|	0|


## RNN 모델  
label을 numpy로 변환해줄 건데 레이블이 단순해서 그냥 하드코딩 했습니다.  
```python
import numpy as np

def label_onehot(label):
  if label ==0 or label == '0':
    return np.array([1,0,0])
  if label ==1 or label == '1':
    return np.array([0,1,0])
  if label ==2 or label == '2':
    return np.array([0,0,1])
  raise KeyError("label value out of range : " + str(label))

data["label_numpy"] = data['label'].apply(lambda x:label_onehot(x))
```
  
다음은 문장을 임베딩해서 수치화합니다.  
Mecab 과 Word2Vec으로 최대한 단순하게 구현했습니다.  

```python
from konlpy.tag import Mecab
mecab-ko-dic_path = "####"
mecab = Mecab(mecab-ko-dic_path) 

def sentence_mecab(sentence):
  output = []
  for pos in mecab.pos(sentence):
    if pos[1] == "SY":
      continue
    output.append(pos[0])
  return output
```
Mecab으로 문장을 형태소 단위로 분리했습니다.  
단순하게 한다지만 그래도 "SY"를 빼서 의미없는 기호는 제외시켰습니다.  

```python
from gensim.models import Word2Vec
model = Word2Vec(sentences=data["Q_mecab"].tolist(),size=200,window=3, min_count=1)
```  
Mecab으로 형태소를 Word2Vec으로 수치화합니다.   
더 좋은 결과가 나오게 하려면  
pretrained model과 결합하면 될 것 같습니다.  

```python
def sentence_embedding(sentence):
  output = []
  for pos in mecab.pos(sentence):
    if pos[1] == "SY":
      continue
    if pos[0] not in model:
      continue
    output.append(model[pos[0]])
  return np.array(output)

data["Q_w2v"] = data["Q"].apply(lambda x:sentence_embedding(x))
```  
생성된 Word2Vec 모델로 문장을 수치화했습니다.  


## Forward
![자연어처리흐름](/assets/img/인공지능/자연어처리흐름.png)  

히든 레이어는 한 개로 구현했습니다.  
학습시켜야 할 변수는 총 5개 입니다.  
input의 bias는 있어도 Bh와 선형결합이기 때문에 필요없습니다.  

```python
def init(input_dim, output_dim):
  np.random.seed(42)
  Bh = np.random.rand(input_dim) - 0.5
  Wh = np.random.rand(input_dim,input_dim) - 0.5
  Wx = np.random.rand(input_dim,input_dim) - 0.5
  Wy = np.random.rand(output_dim,input_dim) - 0.5
  By = np.random.rand(output_dim) - 0.5
  return Wx,Wh,Wy,Bh,By
```  
`np.random.rand`는 `0~1`까지 랜덤한 값으로 나오기 때문에 단순하게 0.5를 뺐습니다.  

```python
def softmax(x):
  return np.exp(x) / sum(np.exp(x))

# def tanh(x): return np.tanh(x)

def cross_entropy_loss(y, y_):
  return sum(-y_ * np.log(y))
```

그림에서 h에서 activation 함수는 tanh, y에서 activation 함수는 softmax 입니다.  
loss function은 cross_entropy_loss 는 쓸 일은 없습니다.  

```python
def forward(Wx, Wh, Wy, Bh, By, X):
  # x.shape = (200,)
  H_out = [np.zeros(shape=(X[0].shape[0]))]
  H_in = []
  for x in X:
    h_in = Wx@x + Wh@H_out[-1] + Bh
    h_out = np.tanh(h_in)
    H_in.append(h_in)
    H_out.append(h_out)
  y_in = Wy@H_out[-1] + By
  y_out = softmax(y_in)
  return H_in, H_out[1:], y_in, y_out
```  
forward는
- X의 길이만큼 재귀적으로 반복, 반복된 결과 `Hout`을 다시 input으로 결합
- 재귀가 끝나면 `Hout`을 y로 전달  

하면 됩니다.  
`H_out[0]`을 0으로 두어도 결과에 영향을 주지 않습니다.  
첫 step은 Hidden layer의 결과를 받을 필요가 없습니다.  
`for문`안에 `if문`을 두지 않고  
첫 행렬에 0을 넣고 `return`시 첫 행렬을 빼고 `return`했습니다.  
`return`된 변수는 backward에서 쓰입니다.  

## Backward
```python
def der_tanh(dA, x):
  tanh = np.tanh(x)
  y = 1 - tanh*tanh
  return dA * y

def der_soft_max_cross_entropy_loss(y_out, y_):
  return y_out - y_

def der_weight(dA, x):
  return x * np.reshape(dA, (dA.shape[0],1))
```  
미분 함수부터 구현했습니다. `der_soft_max_cross_entropy_loss`는 $$\frac{\partial L}{\partial \textbf{y}_{in}}$$를 미분한 결과입니다.  
`der_tanh`와 `der_weight`는 이전 포스트에서 설명한 것 이라 넘어가겠습니다.  

```python
def backward(Wx, Wh, Wy, Bh, By, X, H_in, H_out, y_in, y_out, y_, a):
  Dhin = ["" for _ in range(X.shape[0]-1)]
  Dyin = der_soft_max_cross_entropy_loss(y_out, y_) # (3)

  for i in range(len(Dhin)-1,-1,-1):
    if i==len(Dhin)-1:  # 제일 끝에 있는 Hin
      Dhin[i] = der_tanh(Dyin@Wy, H_in[i]) # (200)
      continue
    Dhin[i] = der_tanh(Dhin[i+1]@Wh, H_in[i]) # (200)
  
  # update Wy
  Wy_new = Wy - a * der_weight(Dyin, H_out[-1])   # (3,200)
  
  # update By
  By_new = By - a * 1 * Dyin

  # update Wx
  Wx_minus = 0
  for i in range(len(Dhin)):
    Wx_minus += der_weight(Dhin[i], X[i])
  Wx_new = Wx - a * Wx_minus

  # update Bh
  Bh_minus = 0
  for i in range(len(Dhin)):
    Bh_minus += 1 * Dhin[i]
  Bh_new = Bh - a * Bh_minus

  # update Wh
  Wh_minus = 0
  for i in range(len(Dhin)-1):
    Wh_minus += der_weight(Dhin[i], H_out[i])
  Wh_new = Wh - a * Wh_minus
  return Wx_new, Wh_new, Wy_new, Bh_new, By_new
```  

$$\frac{\partial L}{\partial \textbf{y}_in}$$와 $$\frac{\partial L}{\partial \textbf{h}_{in}}$$를 단순하게 `[Dyin, Dhin]`로 표현했습니다.  
`Dyin[]`를 먼저 구해야 나중에 계산하기 편합니다.  
`der_tanh(Dyin@Wy, H_in[i])`는  $$\frac{\partial L}{\partial \textbf{y}_in}\frac{\partial \textbf{y}_in}{\partial \textbf{h}_{out}}\frac{\partial \textbf{h}_{out}}{\partial \textbf{h}_in}$$ 입니다.  
참고로 $$\frac{\partial \textbf{y}_{in}}{\partial \textbf{h}_{out}} = W_y$$입니다.  
  
## Train
이제 만든 모델로 학습 해보겠습니다.  
```python
import sys

def is_output_match(y, y_):
  if y_[np.argmax(y)] == 1:
    return True
  return False

class ProgressBar:
  def __init__(self, total, size=50, left = '█', right = '-') -> None:
    self.left = left
    self.right = right
    self.size = size
    self.total = total

  def print_line(self, iteration, prefix=None, suffix=None):
    filled_length = int(self.size * (iteration / self.total))
    bar = self.left * filled_length + self.right * (self.size - filled_length) 
    sys.stdout.write('\r%s |%s| %s' % (prefix, bar, suffix))

  def print_next(self):
    sys.stdout.write('\n')
    sys.stdout.flush()
```
`ProgressBar`와 `is_output_match`로 학습과정을 찍습니다.  

```python
def trainRNN(X, Y, epochs, learning_rate, validation = None):
  
  shuffle = np.arange(X.shape[0])
  np.random.shuffle(shuffle)

  X = X[shuffle]
  Y = Y[shuffle]

  Wx,Wh,Wy,Bh,By = init(X[0].shape[1], Y[0].shape[0])
  if validation is not None:
    X_val_idx = set(np.random.choice([i for i in range(X.shape[0])], int(X.shape[0] * validation), replace=False))
  for epoch in range(epochs):
    progress_bar = ProgressBar(total=len(X))
    val_match_cnt = 0
    train_match_cnt = 0
    val_cnt = 0
    train_cnt = 0
    for i in range(len(X)):
      x = X[i]
      y_ = Y[i]
      if len(x)==0:
        continue
      H_in, H_out, y_in, y_out = forward(Wx,Wh,Wy,Bh,By,x)
      if i in X_val_idx:
        val_cnt += 1
        if is_output_match(y_out,y_):
          val_match_cnt += 1
      else:
        train_cnt += 1
        if is_output_match(y_out,y_):
          train_match_cnt += 1        
        Wx, Wh, Wy, Bh, By = backward(Wx,Wh,Wy,Bh,By,x,H_in,H_out,y_in,y_out,y_,learning_rate)
      progress_bar_prefix = '{}/{} epoch : '.format(epoch+1,epochs)
      progress_bar_suffix = ''
      if train_cnt!=0:
        # progress_bar_suffix += 'train_acc : {}, {}'.format(train_match_cnt,train_cnt)
        progress_bar_suffix += 'train_acc : {:.2f}'.format(train_match_cnt/train_cnt)
      if validation is not None and val_cnt!=0:
        progress_bar_suffix += ', val_acc : {:.2f}'.format(val_match_cnt/val_cnt)
      progress_bar.print_line(i+1, progress_bar_prefix, progress_bar_suffix)
    progress_bar.print_next()
  return Wx,Wh,Wy,Bh,By
```
따로 X_test 없이 validation 만 진행합니다.  

```python
X = np.array(train_data["Q_w2v"].tolist())
Y = np.array(train_data["label_numpy"].tolist())
Wx,Wh,Wy,Bh,By = trainRNN(X,Y,100,0.001, 0.1)
```  
```
...
96/100 epoch :  |██████████████████████████████████████████████████| train_acc : 0.60, val_acc : 0.59
97/100 epoch :  |██████████████████████████████████████████████████| train_acc : 0.59, val_acc : 0.58
98/100 epoch :  |██████████████████████████████████████████████████| train_acc : 0.60, val_acc : 0.59
99/100 epoch :  |██████████████████████████████████████████████████| train_acc : 0.60, val_acc : 0.60
100/100 epoch :  |██████████████████████████████████████████████████| train_acc : 0.60, val_acc : 0.59
```  
레이블이 3개밖에 안되는데 생각만큼 잘 되지는 않네요.  
다음에 좀 더 보완해보도록 하겠습니다.  

## 전체코드
[https://gist.github.com/jhong92-pro/4a90b9eb866446f7cb6567f93b68ec18](https://gist.github.com/jhong92-pro/4a90b9eb866446f7cb6567f93b68ec18)
<script src="https://gist.github.com/jhong92-pro/4a90b9eb866446f7cb6567f93b68ec18.js"></script>